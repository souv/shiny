library(xml2)
library(stringr)
chosePage <- function(starpage,endpage){
Sys.setlocale("LC_ALL", "cht")
part1_url <-"http://www.moneydj.com/forum/showforum-8"
part3_url <- ".aspx"
for(i in starpage:endpage){
if(i==1){
testurl <- paste(part1_url,part3_url,sep="")
}
if(i>=2){
num <- paste("-",i,sep="")
testurl <- paste(part1_url,num,part3_url,sep="")
}
url <- testurl
doc <- read_html(url)
titlexpath <- "//*[@class='subject']//a"
title <- xml_text(xml_find_all(doc, titlexpath))              #title
urlxpath <- "//*[@class='subject']//a"
content_url <- xml_attrs(xml_find_all(doc, urlxpath))
content_url <- gsub(pattern="[[:space:]]",replacement="",x= content_url)
part_url <-"http://www.moneydj.com"
content_url <- paste(part_url,content_url,sep="")            #content_url
authorxpath <-"//*[@class='author']//a"
author <- xml_text(xml_find_all(doc,authorxpath))           #author
timexpath <- "//*[@ class='author']//em"
time <- xml_text(xml_find_all(doc,timexpath))
time <- gsub(pattern="[[:space:]]",replacement="",x= time)   #time
time <- paste(substr(time,1,10),substr(time,11,nchar(time)),sep="_")
rvxpath <- "//*[@ class='nums']"
rv <- xml_text(xml_find_all(doc,rvxpath))                   #rv
content=0
scraping_time=0
for(j in 1:length(content_url)){
url <- content_url[j]
doc <- read_html(url)
contentxpath <-"//*[@id='firstpost']"
contenttemp <- xml_text(xml_find_all(doc,contentxpath))
content[j] <- gsub(pattern="[[:space:]]",replacement="",x= contenttemp)
scraping_time[j] <- gsub(pattern="[[:space:]]",replacement="_",Sys.time())
Sys.sleep(1)
#print(i)
}
if(testurl=="http://www.moneydj.com/forum/showforum-8.aspx"){
author<- author[-1]
time <- time[-1]
rv <- rv[-c(1,2)]      #First page will scrape the hot subject rv  so need if to control
}
if(testurl!="http://www.moneydj.com/forum/showforum-8.aspx"){
rv <- rv[-1]
}
response <- unlist(strsplit(rv,split="/"))[seq(1,length(unlist(strsplit(rv,split="/"))),by=2)]
viewcount <- unlist(strsplit(rv,split="/"))[seq(0,length(unlist(strsplit(rv,split="/"))),by=2)]
title <- as.data.frame(title)
author <- as.data.frame(author)
time <- as.data.frame(time)
content_url <- as.data.frame(content_url)
content <- as.data.frame(content)
scraping_time  <- as.data.frame(scraping_time)
response <- as.data.frame(response)
viewcount <- as.data.frame(viewcount)
MoneyDJForum <- cbind(title,author,time,scraping_time,content_url,content,response,viewcount )
file_name <- paste("./temp/",paste("newsPage_",i,sep=""),".txt",sep="")
write.csv(MoneyDJForum,file=file_name)
}
}
#chosePage(1,2) #input starpage & endpage
rm(list=ls(all=TRUE))
# install.packages("jiebaR")
# install.packages("tm")
# install.packages("slam")
# install.packages("RColorBrewer")
# install.packages("wordcloud")
# install.packages("topicmodels")
# install.packages("igraph")
# install.packages("xml2")
library(jiebaRD)
library(jiebaR)       # 斷詞利器
library(NLP)
library(tm)           # 文字詞彙矩陣運算
library(slam)         # 稀疏矩陣運算
library(RColorBrewer)
library(wordcloud)    # 文字雲
library(topicmodels)  # 主題模型
library(plyr)
source('chosePage.R')
#Sys.setlocale("LC_ALL", "cht")
result = chosePage(1,2)
install.packages("jiebaR")
install.packages("tm")
install.packages("slam")
install.packages("RColorBrewer")
install.packages("wordcloud")
install.packages("topicmodels")
install.packages("igraph")
install.packages("xml2")
install.packages("RColorBrewer")
install.packages("xml2")
library(jiebaRD)
library(jiebaR)       # 斷詞利器
library(NLP)
library(tm)           # 文字詞彙矩陣運算
library(slam)         # 稀疏矩陣運算
library(RColorBrewer)
library(wordcloud)    # 文字雲
library(topicmodels)  # 主題模型
library(plyr)
source('chosePage.R')
result = chosePage(1,2)
orgPath = "./temp"
text = Corpus(DirSource(orgPath), list(language = NA))
text <- tm_map(text, removePunctuation)
text <- tm_map(text, removeNumbers)
text <- tm_map(text, function(word)
{ gsub("[A-Za-z0-9]", "", word) })
# 進行中文斷詞
mixseg = worker()
mat <- matrix( unlist(text) )
totalSegment = data.frame()
for( j in 1:length(mat) )
{
for( i in 1:length(mat[j,]) )
{
result = segment(as.character(mat[j,i]), jiebar=mixseg)
}
totalSegment = rbind(totalSegment, data.frame(result))
}
# define text array that you want
# delete text length < 2
delidx = which( nchar(as.vector(totalSegment[,1])) < 2 )
countText = totalSegment[-delidx,]
countResult = count(countText)[,1]
countFreq = count(countText)[,2] / sum(count(countText)[,2])
wordcloud(countResult, countFreq, min.freq = 1, random.order = F, ordered.colors = T,
colors = rainbow(length(countResult)))
result
result = chosePage(1,2)
library(xml2)
library(stringr)
startpage=1
endpage=3
chosePage <- function(starpage,endpage){
Sys.setlocale("LC_ALL", "cht")
part1_url <-"http://www.moneydj.com/forum/showforum-8"
part3_url <- ".aspx"
for(i in starpage:endpage){
if(i==1){
testurl <- paste(part1_url,part3_url,sep="")
}
if(i>=2){
num <- paste("-",i,sep="")
testurl <- paste(part1_url,num,part3_url,sep="")
}
url <- testurl
doc <- read_html(url)
titlexpath <- "//*[@class='subject']//a"
title <- xml_text(xml_find_all(doc, titlexpath))              #title
urlxpath <- "//*[@class='subject']//a"
content_url <- xml_attrs(xml_find_all(doc, urlxpath))
content_url <- gsub(pattern="[[:space:]]",replacement="",x= content_url)
part_url <-"http://www.moneydj.com"
content_url <- paste(part_url,content_url,sep="")            #content_url
authorxpath <-"//*[@class='author']//a"
author <- xml_text(xml_find_all(doc,authorxpath))           #author
timexpath <- "//*[@ class='author']//em"
time <- xml_text(xml_find_all(doc,timexpath))
time <- gsub(pattern="[[:space:]]",replacement="",x= time)   #time
time <- paste(substr(time,1,10),substr(time,11,nchar(time)),sep="_")
rvxpath <- "//*[@ class='nums']"
rv <- xml_text(xml_find_all(doc,rvxpath))                   #rv
content=0
scraping_time=0
for(j in 1:length(content_url)){
url <- content_url[j]
doc <- read_html(url)
contentxpath <-"//*[@id='firstpost']"
contenttemp <- xml_text(xml_find_all(doc,contentxpath))
content[j] <- gsub(pattern="[[:space:]]",replacement="",x= contenttemp)
scraping_time[j] <- gsub(pattern="[[:space:]]",replacement="_",Sys.time())
Sys.sleep(1)
#print(i)
}
if(testurl=="http://www.moneydj.com/forum/showforum-8.aspx"){
author<- author[-1]
time <- time[-1]
rv <- rv[-c(1,2)]      #First page will scrape the hot subject rv  so need if to control
}
if(testurl!="http://www.moneydj.com/forum/showforum-8.aspx"){
rv <- rv[-1]
}
response <- unlist(strsplit(rv,split="/"))[seq(1,length(unlist(strsplit(rv,split="/"))),by=2)]
viewcount <- unlist(strsplit(rv,split="/"))[seq(0,length(unlist(strsplit(rv,split="/"))),by=2)]
title <- as.data.frame(title)
author <- as.data.frame(author)
time <- as.data.frame(time)
content_url <- as.data.frame(content_url)
content <- as.data.frame(content)
scraping_time  <- as.data.frame(scraping_time)
response <- as.data.frame(response)
viewcount <- as.data.frame(viewcount)
MoneyDJForum <- cbind(title,author,time,scraping_time,content_url,content,response,viewcount )
file_name <- paste("./temp/",paste("newsPage_",i,sep=""),".txt",sep="")
write.csv(MoneyDJForum,file=file_name)
}
}
#chosePage(1,2) #input starpage & endpage
getwd()
MoneyDJForum
file_name
endpage
chosePage(1,3)
chosePage(1,2) #input starpage & endpage
chosePage
chosePage(1,2) #input starpage & endpage
rm(list=ls(all=TRUE))
# install.packages("jiebaR")
# install.packages("tm")
# install.packages("slam")
# install.packages("RColorBrewer")
# install.packages("wordcloud")
# install.packages("topicmodels")
# install.packages("igraph")
# install.packages("xml2")
library(jiebaRD)
library(jiebaR)       # 斷詞利器
library(NLP)
library(tm)           # 文字詞彙矩陣運算
library(slam)         # 稀疏矩陣運算
library(RColorBrewer)
library(wordcloud)    # 文字雲
library(topicmodels)  # 主題模型
library(plyr)
source('chosePage.R')
#Sys.setlocale("LC_ALL", "cht")
result = chosePage(1,2)
chosePage
result = chosePage(1,2)
result = chosePage(1,2)
result
result = chosePage(1,2)
orgPath = "./temp"
text = Corpus(DirSource(orgPath), list(language = NA))
text <- tm_map(text, removePunctuation)
text <- tm_map(text, removeNumbers)
text <- tm_map(text, function(word)
{ gsub("[A-Za-z0-9]", "", word) })
mixseg = worker()
mat <- matrix( unlist(text) )
totalSegment = data.frame()
for( j in 1:length(mat) )
{
for( i in 1:length(mat[j,]) )
{
result = segment(as.character(mat[j,i]), jiebar=mixseg)
}
totalSegment = rbind(totalSegment, data.frame(result))
}
delidx = which( nchar(as.vector(totalSegment[,1])) < 2 )
countText = totalSegment[-delidx,]
countResult = count(countText)[,1]
countFreq = count(countText)[,2] / sum(count(countText)[,2])
wordcloud(countResult, countFreq, min.freq = 1, random.order = F, ordered.colors = T,
colors = rainbow(length(countResult)))
text
?Corpus
?DirSource
text = Corpus(DirSource(orgPath), list(language = NA))
text
class(text)
text <- tm_map(text, removePunctuation)
text
class(text)
?tm_map
text <- tm_map(text, removeNumbers)
text <- tm_map(text, function(word)
{ gsub("[A-Za-z0-9]", "", word) })
mixseg = worker()
mixseg
text
class(text)
unlist(text)
totalSegment = data.frame()
mat
class(unlist(text))
mat[1,]
mat[2,]
mat[3,]
mat[4,]
?segement
?segment
mat[2,2]
mat[2,3]
mat[1,3]
mat[1,3]
length(mat)
length(mat[42,])
length(mat[41,])
length(mat[40,])
mat
mat[41,]
mat[,41]
mat[,1]
mat[,2]
mat[41,1]
mat[39,1]
mat[39,]
mat[39,2]
length(mat[j,])
length(mat[5,])
length(mat[8,])
class(mat)
for( j in 1:length(mat) )
{
for( i in 1:length(mat[j,]) )
{
result = segment(as.character(mat[j,i]), jiebar=mixseg)
}
totalSegment = rbind(totalSegment, data.frame(result))
}
totalSegment
result
?segment
delidx = which( nchar(as.vector(totalSegment[,1])) < 2 )
delidx
?whicj
?which
totalSegement
totalSegment[,1]
which(LERRERS=="R")
which(LETTERS=="R")
which(11)
countText = totalSegment[-delidx,]#減去長度小於2的關鍵字的編號
countText
?count
count(countText)
count(countText)[,1]
countResult = count(countText)[,1]
countResult
countFreq = count(countText)[,2] / sum(count(countText)[,2])
count(countText)
count(countText)[,2]
count(countText)[,1]
countFreq
countResult
orgPath = "./open_stock"
text = Corpus(DirSource(orgPath), list(language = NA))
orgPath = "./open_stock"
text = Corpus(DirSource(orgPath), list(language = NA))
text
text <- tm_map(text, removePunctuation)
unlist(text)
text <- tm_map(text, removeNumbers)
text <- tm_map(text, function(word)
{ gsub("[A-Za-z0-9]", "", word) })
mixseg = worker()
mat <- matrix( unlist(text) )#把character用成matrix
may
mat
totalSegment = data.frame()
for( j in 1:length(mat) )
{
for( i in 1:length(mat[j,]) )
{
result = segment(as.character(mat[j,i]), jiebar=mixseg)
}
totalSegment = rbind(totalSegment, data.frame(result))
}
totalSegment
delidx = which( nchar(as.vector(totalSegment[,1])) < 2 )#用which來做挑選編號
countText = totalSegment[-delidx,]#減去長度小於2的關鍵字的編號
countResult = count(countText)[,1]#要做文字雲的那些關鍵字
countFreq = count(countText)[,2] / sum(count(countText)[,2]) #所占的比例
wordcloud(countResult, countFreq, min.freq = 5, random.order = F, ordered.colors = T,
colors = rainbow(length(countResult)))
myTdm <- TermDocumentMatrix(text)
tdm <- TermDocumentMatrix(text, control = list(wordLengths = c(2, Inf)))
orgPath = "./temp"
text = Corpus(DirSource(orgPath), list(language = NA))
text <- tm_map(text, removePunctuation)
text <- tm_map(text, removeNumbers)
text <- tm_map(text, function(word)
{ gsub("[A-Za-z0-9]", "", word) })
# 進行中文斷詞
mixseg = worker()
mat <- matrix( unlist(text) )#把character用成matrix
totalSegment = data.frame()
#用雙重迴圈把矩陣轉換成資料表，當中順便做了斷詞
for( j in 1:length(mat) )
{
for( i in 1:length(mat[j,]) )
{
result = segment(as.character(mat[j,i]), jiebar=mixseg)
}
totalSegment = rbind(totalSegment, data.frame(result))
}
tdm <- TermDocumentMatrix(text, control = list(wordLengths = c(2, Inf)))
traceback()
remove.packages(tm)
remove.packages("tm")
install.packages("http://cran.r-project.org/bin/windows/contrib/3.0/tm_0.5-10.zip",repos=NULL)
install.packages("http://cran.r-project.org/bin/windows/contrib/3.0/tm_0.5-10.zip",repos=NULL)
library(tm)
orgPath = "./open_stock"
text = Corpus(DirSource(orgPath), list(language = NA))
library(jiebaRD)
library(jiebaR)       # 斷詞利器
library(NLP)
library(tm)           # 文字詞彙矩陣運算
library(slam)         # 稀疏矩陣運算
library(RColorBrewer)
library(wordcloud)    # 文字雲
# library(topicmodels)  # 主題模型
library(plyr)
text = Corpus(DirSource(orgPath), list(language = NA))
